{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "Setting up some basic packages for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in training and testing data. Mapping salary to binary value (this is what we want to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14160</th>\n",
       "      <td>29</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27048</th>\n",
       "      <td>19</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28868</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>35</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7827</th>\n",
       "      <td>20</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>27</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>1573</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24534</th>\n",
       "      <td>63</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18080</th>\n",
       "      <td>56</td>\n",
       "      <td>Private</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10354</th>\n",
       "      <td>22</td>\n",
       "      <td>?</td>\n",
       "      <td>Assoc-voc</td>\n",
       "      <td>11</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24639</th>\n",
       "      <td>45</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6513 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass     education  education.num      marital.status  \\\n",
       "14160   29           Private  Some-college             10  Married-civ-spouse   \n",
       "27048   19           Private  Some-college             10       Never-married   \n",
       "28868   28           Private  Some-college             10  Married-civ-spouse   \n",
       "5667    35           Private          11th              7       Never-married   \n",
       "7827    20           Private  Some-college             10       Never-married   \n",
       "...    ...               ...           ...            ...                 ...   \n",
       "1338    27  Self-emp-not-inc       HS-grad              9  Married-civ-spouse   \n",
       "24534   63           Private       HS-grad              9       Never-married   \n",
       "18080   56           Private       Masters             14  Married-civ-spouse   \n",
       "10354   22                 ?     Assoc-voc             11       Never-married   \n",
       "24639   45  Self-emp-not-inc       7th-8th              4  Married-civ-spouse   \n",
       "\n",
       "              occupation   relationship     sex  capital.gain  capital.loss  \\\n",
       "14160  Handlers-cleaners        Husband    Male             0             0   \n",
       "27048              Sales      Own-child    Male             0             0   \n",
       "28868       Tech-support           Wife  Female             0             0   \n",
       "5667               Sales  Not-in-family    Male             0             0   \n",
       "7827        Adm-clerical      Own-child  Female             0             0   \n",
       "...                  ...            ...     ...           ...           ...   \n",
       "1338     Farming-fishing        Husband    Male             0          1573   \n",
       "24534    Exec-managerial  Not-in-family    Male             0             0   \n",
       "18080     Prof-specialty        Husband    Male             0             0   \n",
       "10354                  ?      Own-child    Male             0             0   \n",
       "24639    Exec-managerial        Husband    Male             0             0   \n",
       "\n",
       "       hours.per.week  \n",
       "14160              40  \n",
       "27048              15  \n",
       "28868              40  \n",
       "5667               45  \n",
       "7827               30  \n",
       "...               ...  \n",
       "1338               70  \n",
       "24534              40  \n",
       "18080              40  \n",
       "10354              20  \n",
       "24639              65  \n",
       "\n",
       "[6513 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "df = pd.read_csv(\"adult-data.csv\")\n",
    "train, test = train_test_split(df, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "train['income'] = train['income'].map({\"<=50K\":0, \">50K\":1})\n",
    "test['income'] = test['income'].map({\"<=50K.\":0, \">50K.\":1})\n",
    "# To binary mapping\n",
    "train['race'] = train['race'].map({\"Black\": 0, \"Other\": 1, \"Amer-Indian-Eskimo\": 1, \"Asian-Pac-Islander\": 1, \"White\":1})\n",
    "test['race'] = test['race'].map({\"Black\": 0, \"Other\": 1, \"Amer-Indian-Eskimo\": 1, \"Asian-Pac-Islander\": 1, \"White\":1})\n",
    "\n",
    "Y = train['income']\n",
    "Y_race = train['race']\n",
    "X = train.drop(['income', 'fnlwgt', 'native.country', 'race'], axis = 1)\n",
    "Y_test = test['income']\n",
    "Y_test_race = test['race']\n",
    "X_test = test.drop(['income', 'fnlwgt', 'native.country', 'race'], axis = 1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive oversampling of our training data. This one of the mechanisms we will test to see its effect on bias. We simply oversample the minority class (black in this case) so that our classes are more balanced. We are going to resample based on some coefficient RESAMPLE_COEFFICIENT. If this is 1, we simple resample the total amount in the minority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3124 32561\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical, plot_model\n",
    "\n",
    "minority = train[train['race']== 0]\n",
    "print(len(minority), len(train))\n",
    "RESAMPLE_COEFFICIENT = 1.5\n",
    "oversample = minority.sample(int(RESAMPLE_COEFFICIENT * len(minority)), replace = True, random_state = 1)\n",
    "train_o = pd.concat([train, oversample], axis = 0)\n",
    "##Perform similar X, Y split on train_o, the oversampled training data\n",
    "Y_o_race = train_o['race']\n",
    "Y_o = train_o['ann_salary']\n",
    "X_o = train_o.drop(['ann_salary', 'fnlwgt', 'country', 'race'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic analysis of race groups above and below 50 K (\"rich\" and \"poor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = train.groupby('race').size().reset_index()\n",
    "\n",
    "rich = train[train['ann_salary'] == 1]\n",
    "rich = rich.groupby('race').size().reset_index()\n",
    "rich[0] = rich[0]/total[0]\n",
    "\n",
    "poor = train[train['ann_salary'] == 0]\n",
    "poor = poor.groupby('race').size().reset_index()\n",
    "poor[0] = poor[0]/total[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_size_inches(10,3)\n",
    "ax1.bar(rich['race'], rich[0])\n",
    "ax2.bar(poor['race'], poor[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see what is the value of p in our training data, because it is likely that such a pattern will reveal itself in the model. In other words, what is the maximum bias that is shown in the data? This is assuming there are no confounding variables between race groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proportion of black people who are rich. Dividing the \"length\" of the DFs of only >50k and the entire data, subsetting for black race\n",
    "blackrich = len(rich[rich['race'] == 0])/len(train[train['race'] == 0])\n",
    "#Same idea for other races\n",
    "nonblackrich = len(rich[rich['race'] == 1])/len(train[train['race'] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More processing things, one-hot encoding categorical variables, making sure that the test and train have the same encodings, by using df.align on column case. Then, converting everything into numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37247, 68)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras import Sequential\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "dumdf = pd.get_dummies(X, columns = [\"workclass\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"sex\"], drop_first = True)\n",
    "testdf = pd.get_dummies(X_test, columns = [\"workclass\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"sex\"], drop_first = True)\n",
    "dumdf_o = pd.get_dummies(X_o, columns = [\"workclass\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"sex\"], drop_first =True)\n",
    "dumdf,testdf = dumdf.align(testdf, join='outer', axis=1, fill_value=0)\n",
    "dumdf_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures = dumdf.shape[1]\n",
    "\n",
    "#Regular Dataframes\n",
    "dumdf = np.asarray(dumdf)\n",
    "Y = np.asarray(Y)\n",
    "Y_race = np.asarray(Y_race)\n",
    "\n",
    "#Oversampled Dataframes\n",
    "dumdf_o = np.asarray(dumdf_o)\n",
    "Y_o = np.asarray(Y_o)\n",
    "Y_o_race = np.asarray(Y_o_race)\n",
    "\n",
    "#Testing Dataframes\n",
    "testdf = np.array(testdf)\n",
    "Y_test = np.array(Y_test)\n",
    "Y_test_race = np.array(Y_test_race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin ML pipeline. Setting up a basic neural network structure (which can be tinkered with later). \n",
    "\n",
    "The idea is we are trying to predict M3 (average income), but at the same time we want to discriminate against predicting C2 (race). \n",
    "\n",
    "So the rough structure looks like: M1 -> M2 -> M3 (income) -> C2 -> C3 (race)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Input, Model\n",
    "M1 = Input(shape=(nfeatures,),name='M1')\n",
    "M2 = Dense(400, activation='relu',name='M2')(M1)\n",
    "M3 = Dense(1, activation='sigmoid',name='M3')(M2)\n",
    "C2 = Dense(40, activation='relu',name='C2')(M3)\n",
    "C3 = Dense(1, activation='sigmoid',name='C3')(C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model with adjusted loss function. Define some value $alpha$ that represents the \"discriminator\" network, that punishes the loss function when the loss for C3 is smaller. $alpha$ is an important hyperparamater to choose, though, since a large $alpha$ may drive the network away from any good convergence, while a small $alpha$ does little to nothing (obviously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "814/814 [==============================] - 2s 2ms/step - loss: 2.2109 - M3_loss: 2.2876 - C3_loss: 0.7671 - M3_accuracy: 0.8149 - C3_accuracy: 0.0951 - val_loss: 0.3625 - val_M3_loss: 0.4518 - val_C3_loss: 0.8933 - val_M3_accuracy: 0.8478 - val_C3_accuracy: 0.0993\n",
      "Epoch 2/5\n",
      "814/814 [==============================] - 2s 2ms/step - loss: 1.1542 - M3_loss: 1.2443 - C3_loss: 0.9015 - M3_accuracy: 0.8268 - C3_accuracy: 0.0951 - val_loss: 1.3040 - val_M3_loss: 1.3939 - val_C3_loss: 0.8989 - val_M3_accuracy: 0.8181 - val_C3_accuracy: 0.0993\n",
      "Epoch 3/5\n",
      "814/814 [==============================] - 2s 2ms/step - loss: 1.3472 - M3_loss: 1.4376 - C3_loss: 0.9041 - M3_accuracy: 0.8285 - C3_accuracy: 0.0951 - val_loss: 0.8774 - val_M3_loss: 0.9674 - val_C3_loss: 0.9002 - val_M3_accuracy: 0.8308 - val_C3_accuracy: 0.0993\n",
      "Epoch 4/5\n",
      "814/814 [==============================] - 1s 2ms/step - loss: 1.7845 - M3_loss: 1.8750 - C3_loss: 0.9046 - M3_accuracy: 0.8305 - C3_accuracy: 0.0951 - val_loss: 1.7594 - val_M3_loss: 1.8495 - val_C3_loss: 0.9004 - val_M3_accuracy: 0.8148 - val_C3_accuracy: 0.0993\n",
      "Epoch 5/5\n",
      "814/814 [==============================] - 1s 2ms/step - loss: 1.4012 - M3_loss: 1.4917 - C3_loss: 0.9048 - M3_accuracy: 0.8341 - C3_accuracy: 0.0951 - val_loss: 1.0223 - val_M3_loss: 1.1124 - val_C3_loss: 0.9005 - val_M3_accuracy: 0.8300 - val_C3_accuracy: 0.0993\n"
     ]
    }
   ],
   "source": [
    "merged = Model(inputs = [M1], outputs = [M3, C3])\n",
    "loss1 = 'binary_crossentropy'\n",
    "loss2 = 'mse'\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "alpha = 0.1\n",
    "merged.compile(optimizer = opt,loss=[loss1, loss2],loss_weights=[1, -1 * alpha], metrics=['accuracy'])\n",
    "history = merged.fit(dumdf, [Y,Y_race] ,batch_size=32, epochs=5, validation_split = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Model\n",
    "Just a typical neural network, without any of the debiasing features. This will serve as a benchmark for some of the metrics later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "814/814 [==============================] - 2s 2ms/step - loss: 4.3628 - accuracy: 0.8126 - val_loss: 1.2376 - val_accuracy: 0.8291\n",
      "Epoch 2/5\n",
      "814/814 [==============================] - 1s 2ms/step - loss: 1.9147 - accuracy: 0.8289 - val_loss: 0.5938 - val_accuracy: 0.8267\n",
      "Epoch 3/5\n",
      "814/814 [==============================] - 1s 2ms/step - loss: 1.2276 - accuracy: 0.8340 - val_loss: 0.5823 - val_accuracy: 0.8359\n",
      "Epoch 4/5\n",
      "814/814 [==============================] - 1s 2ms/step - loss: 0.7865 - accuracy: 0.8421 - val_loss: 0.4445 - val_accuracy: 0.8509\n",
      "Epoch 5/5\n",
      "814/814 [==============================] - 1s 2ms/step - loss: 0.9236 - accuracy: 0.8375 - val_loss: 0.4537 - val_accuracy: 0.8488\n"
     ]
    }
   ],
   "source": [
    "\n",
    "M1 = Input(shape=(nfeatures,),name='M1')\n",
    "M2 = Dense(400, activation='relu',name='M2')(M1)\n",
    "M3 = Dense(1, activation='sigmoid',name='M3')(M2)\n",
    "\n",
    "\n",
    "model = Model(inputs = [M1], outputs = [M3])\n",
    "model.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = ['accuracy'])\n",
    "history = model.fit(dumdf,Y, batch_size = 32, epochs = 5, validation_split = 0.2, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampled Model\n",
    "Model with the oversampled training set, same neural network structure as the normal model though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 3.5301 - accuracy: 0.8134 - val_loss: 0.8695 - val_accuracy: 0.8142\n",
      "Epoch 2/5\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 1.2402 - accuracy: 0.8321 - val_loss: 0.4636 - val_accuracy: 0.8737\n",
      "Epoch 3/5\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 1.0984 - accuracy: 0.8379 - val_loss: 0.4508 - val_accuracy: 0.8791\n",
      "Epoch 4/5\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.9706 - accuracy: 0.8388 - val_loss: 0.5021 - val_accuracy: 0.8758\n",
      "Epoch 5/5\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.9577 - accuracy: 0.8399 - val_loss: 0.3749 - val_accuracy: 0.8886\n"
     ]
    }
   ],
   "source": [
    "M1 = Input(shape=(nfeatures,),name='M1')\n",
    "M2 = Dense(400, activation='relu',name='M2')(M1)\n",
    "M3 = Dense(1, activation='sigmoid',name='M3')(M2)\n",
    "\n",
    "\n",
    "Omodel = Model(inputs = [M1], outputs = [M3])\n",
    "Omodel.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = ['accuracy'])\n",
    "history = Omodel.fit(dumdf_o,Y_o, batch_size = 32, epochs = 5, validation_split = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling + Combined Loss Function\n",
    "\n",
    "This one is supposed to combine our previous two insights. We expect this model to perform the \"best\" based on our ideas of debiasing. But anything is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 2.8908 - M3_loss: 2.9679 - C3_loss: 0.7709 - M3_accuracy: 0.8141 - C3_accuracy: 0.0959 - val_loss: 0.9468 - val_M3_loss: 0.9801 - val_C3_loss: 0.3326 - val_M3_accuracy: 0.8748 - val_C3_accuracy: 0.6648\n",
      "Epoch 2/5\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 1.6992 - M3_loss: 1.7893 - C3_loss: 0.9007 - M3_accuracy: 0.8262 - C3_accuracy: 0.0959 - val_loss: 0.4859 - val_M3_loss: 0.5193 - val_C3_loss: 0.3346 - val_M3_accuracy: 0.8815 - val_C3_accuracy: 0.6648\n",
      "Epoch 3/5\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 1.1107 - M3_loss: 1.2010 - C3_loss: 0.9032 - M3_accuracy: 0.8308 - C3_accuracy: 0.0959 - val_loss: 0.5913 - val_M3_loss: 0.6248 - val_C3_loss: 0.3349 - val_M3_accuracy: 0.8713 - val_C3_accuracy: 0.6648\n",
      "Epoch 4/5\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 2.3927 - M3_loss: 2.4830 - C3_loss: 0.9037 - M3_accuracy: 0.8308 - C3_accuracy: 0.0959 - val_loss: 0.4982 - val_M3_loss: 0.5317 - val_C3_loss: 0.3351 - val_M3_accuracy: 0.8860 - val_C3_accuracy: 0.6648\n",
      "Epoch 5/5\n",
      "932/932 [==============================] - 2s 2ms/step - loss: 0.8411 - M3_loss: 0.9315 - C3_loss: 0.9039 - M3_accuracy: 0.8390 - C3_accuracy: 0.0959 - val_loss: 1.6397 - val_M3_loss: 1.6732 - val_C3_loss: 0.3351 - val_M3_accuracy: 0.8783 - val_C3_accuracy: 0.6648\n"
     ]
    }
   ],
   "source": [
    "M1 = Input(shape=(nfeatures,),name='M1')\n",
    "M2 = Dense(400, activation='relu',name='M2')(M1)\n",
    "M3 = Dense(1, activation='sigmoid',name='M3')(M2)\n",
    "C2 = Dense(40, activation='relu',name='C2')(M3)\n",
    "C3 = Dense(1, activation='sigmoid',name='C3')(C2)\n",
    "\n",
    "Omerged = Model(inputs = [M1], outputs = [M3, C3])\n",
    "loss1 = 'binary_crossentropy'\n",
    "loss2 = 'mse'\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "alpha = 0.1\n",
    "Omerged.compile(optimizer = opt,loss=[loss1, loss2],loss_weights=[1, -1 * alpha], metrics=['accuracy'])\n",
    "history = Omerged.fit(dumdf_o, [Y_o,Y_o_race] ,batch_size=32, epochs=5, validation_split = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the models on test set\n",
    "scores_merged = merged.evaluate(testdf, [Y_test, Y_test_race], batch_size=16, verbose = 0)\n",
    "scores = model.evaluate(testdf, Y_test, batch_size = 16, verbose = 0)\n",
    "scores_o = Omodel.evaluate(testdf, Y_test, batch_size = 16, verbose = 0)\n",
    "scores_merged_o = Omerged.evaluate(testdf, [Y_test, Y_test_race], batch_size=16, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Loss Function Model: \n",
      "Loss: 1.2430120706558228\n",
      "Accuracy: 0.8312757015228271\n",
      "Simple Model: \n",
      "Loss: 0.495622456073761\n",
      "Accuracy: 0.8431914448738098\n",
      "Oversampled Model: \n",
      "Loss: 0.4970332980155945\n",
      "Accuracy: 0.8477980494499207\n",
      "Oversampling + Combined Loss Function Model: \n",
      "Loss: 1.9590778350830078\n",
      "Accuracy: 0.8306615352630615\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Loss Function Model: \\nLoss: \" + str(scores_merged[0]) + \"\\nAccuracy: \" + str(scores_merged[3]))\n",
    "\n",
    "print(\"Simple Model: \\nLoss: \" + str(scores[0]) + \"\\nAccuracy: \" + str(scores[1]))\n",
    "\n",
    "print(\"Oversampled Model: \\nLoss: \" + str(scores_o[0]) + \"\\nAccuracy: \" + str(scores_o[1]))\n",
    "\n",
    "print(\"Oversampling + Combined Loss Function Model: \\nLoss: \" + str(scores_merged_o[0]) + \"\\nAccuracy: \" + str(scores_merged_o[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'M3_loss', 'C3_loss', 'M3_accuracy', 'C3_accuracy']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
